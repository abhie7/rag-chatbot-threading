{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 1536\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alois/Abhiraj/16_RFP_Chatbot/RFP-automation/rfp-chatbot/backend/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash_document_content(content):\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(r\"../documents\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: c3c10254-b0ae-4e57-b6ca-eb420b932957\n",
      "Text: QUESTIONS Are there any mandatory certifications or\n",
      "qualifications that the team needs? Are there any specific technology\n",
      "or security standards that must be met? Does the proposal address\n",
      "local, state, or federal compliance requirements? Are there specific\n",
      "response times or deadlines for deliverables? Is there a staffing or\n",
      "personnel requirement...\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# load embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import threading\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# from app.custom_classes.recursive_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import threading\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from app.custom_classes.recursive_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.schema import TextNode\n",
    "from app.services.doc_parser import DocumentParser\n",
    "\n",
    "def hash_document_content(content):\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def doc_parser_instance(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(('.pdf', '.doc', '.docx')):\n",
    "            file_path = os.path.join(directory, file)\n",
    "\n",
    "            parser = DocumentParser(file_path)\n",
    "            cleaned_text = parser.get_cleaned_text()\n",
    "            \n",
    "            if cleaned_text:\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                print(f\"Failed to parse {file}.\\n\")\n",
    "\n",
    "def create_vector_db(doc_file_path, model_name, db_path):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    documents = []\n",
    "\n",
    "    with open(doc_file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "        chunks = text_splitter.split_text(data)\n",
    "        for chunk in chunks:\n",
    "            node = TextNode(text=chunk, metadata={\"source\": doc_file_path})\n",
    "            documents.append(node)\n",
    "\n",
    "    faiss_index = FaissVectorStore.from_nodes(documents, embed_model=embed_model)\n",
    "    faiss_index.save(db_path)\n",
    "    print(f\"Vector DB created and saved to {db_path}\")\n",
    "\n",
    "def similarity_search(embedding, db_path):\n",
    "    faiss_index = FaissVectorStore.load(db_path)\n",
    "    results = faiss_index.similarity_search(embedding)\n",
    "    print(\"Similarity search results:\", results)\n",
    "    return results\n",
    "\n",
    "def process_document(content, model_name, base_db_path):\n",
    "\n",
    "    doc_hash = hash_document_content(content)\n",
    "    db_path = os.path.join(base_db_path, f\"{doc_hash}.faiss\")\n",
    "\n",
    "    if not os.path.exists(db_path):\n",
    "        thread1 = threading.Thread(target=create_vector_db, args=(content, model_name, db_path))\n",
    "        thread1.start()\n",
    "        thread1.join()\n",
    "    else:\n",
    "        print(f\"Vector DB for {content[:10]} already exists at {db_path}\")\n",
    "\n",
    "    example_embedding = [0.1, 0.2, 0.3]\n",
    "    thread2 = threading.Thread(target=similarity_search, args=(example_embedding, db_path))\n",
    "    thread2.start()\n",
    "    thread2.join()\n",
    "\n",
    "def main():\n",
    "    doc_directory = r\"../documents\"\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    base_db_path = \"vectorDB\"\n",
    "\n",
    "    if not os.path.exists(base_db_path):\n",
    "        os.makedirs(base_db_path)\n",
    "\n",
    "    doc_parser_instance(doc_directory)\n",
    "    process_document(os.path.join(doc_directory, doc_file_path), model_name, base_db_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alois/Abhiraj/16_RFP_Chatbot/RFP-automation/rfp-chatbot/backend/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(r\"../documents\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text or \"\"  \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from PDF: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(r'../documents'):\n",
    "    if file.endswith(('.pdf', '.doc', '.docx')):\n",
    "        file_path = os.path.join(r'../documents', file)\n",
    "        try:\n",
    "            cleaned_text = extract_text_from_pdf(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {file}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"QUESTIONS\\nAre there any mandatory certifications or qualifications that the team needs?\\nAre there any specific technology or security standards that must be met?\\nDoes the proposal address local, state, or federal compliance requirements?\\nAre there specific response times or deadlines for deliverables?\\nIs there a staffing or personnel requirement (number of staff, skills, etc.)?\\nIs there a pricing or cost-related compliance requirement?\\nDoes the proposal include all necessary legal and regulatory documentation (insurance, licenses, etc.)?\\nAre there environmental or sustainability requirements?\\nAre there any Diversity, Equity, and Inclusion (DEI) obligations?\\nAre client reference checks or past performance verifications required?ANSWER\\nThe RFP does not explicitly mention any mandatory certifications or qualifications. \\nHowever, experience with public-sector clients, preferably in aviation or transportation, is emphasized\\nFirms should demonstrate applicable experience in these areas to be considered qualified.\\nThe approach needs to protect sensitive data by adhering to modern cybersecurity best practices. Section 508 of \\nthe Rehabilitation Act Amendments, which requires accessibility for people with impairments, must also be \\nfollowed by the finished website. The websites also need to adhere to the W3C's Web Content Accessibility \\nGuidelines (WCAG). [intuitive, frictionless user interface,online payment capabilities, include social media \\nwidgets or links and compatible with compatible all popular browsers and devices]\\nstate\\nquestions should be submitted by  September 24, 2024 , and\\nthe whole proposal should be submitted on  September 30, 2024 . \\nThere are no specified dates for project deliverables,\\nbut the proposal should outline a timeline for project phases,\\nsuch as pre-design, design, content migration, and maintenance.\\nThe RFP does not specify the exact number of staff required, it emphasizes the need for personnel with relevant \\nskills in web design, development, and maintenance. The selected firm must provide training for up to 10 airport \\nemployees starting from the go-live date, indicating that the team should have the capability to train and \\nsupport airport staff in website management.\\nThe RFP does not outline specific pricing compliance requirements.\\n RFP does not explicitly mention the requirement for specific legal and regulatory documentation, such as \\ninsurance or licenses, within the proposal. \\nRFP does not specify any mandatory environmental or sustainability requirements.\\nIt does not state any explicit DEI obligations. However, given the increasing focus on diversity in public contracts,\\n firms may wish to highlight their commitment to DEI in their proposal to strengthen their bid.\\nIt does not specify that client reference checks or past performance verifications are required.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_split_text(content):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m----> 3\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m [Document(text\u001b[38;5;241m=\u001b[39mchunk, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: file_path}) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m, in \u001b[0;36mrecursive_split_text\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecursive_split_text\u001b[39m(content):\n\u001b[0;32m----> 2\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mRecursiveCharacterTextSplitter\u001b[49m(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(content)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "chunks = recursive_split_text(cleaned_text)\n",
    "\n",
    "documents = [Document(text=chunk, metadata={\"source\": file_path}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='e8f223f6-8d16-4065-a44b-157f002819cd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"QUESTIONS\\nAre there any mandatory certifications or qualifications that the team needs?\\nAre there any specific technology or security standards that must be met?\\nDoes the proposal address local, state, or federal compliance requirements?\\nAre there specific response times or deadlines for deliverables?\\nIs there a staffing or personnel requirement (number of staff, skills, etc.)?\\nIs there a pricing or cost-related compliance requirement?\\nDoes the proposal include all necessary legal and regulatory documentation (insurance, licenses, etc.)?\\nAre there environmental or sustainability requirements?\\nAre there any Diversity, Equity, and Inclusion (DEI) obligations?\\nAre client reference checks or past performance verifications required?ANSWER\\nThe RFP does not explicitly mention any mandatory certifications or qualifications. \\nHowever, experience with public-sector clients, preferably in aviation or transportation, is emphasized\\nFirms should demonstrate applicable experience in these areas to be considered qualified.\\nThe approach needs to protect sensitive data by adhering to modern cybersecurity best practices. Section 508 of \\nthe Rehabilitation Act Amendments, which requires accessibility for people with impairments, must also be \\nfollowed by the finished website. The websites also need to adhere to the W3C's Web Content Accessibility \\nGuidelines (WCAG). [intuitive, frictionless user interface,online payment capabilities, include social media \\nwidgets or links and compatible with compatible all popular browsers and devices]\\nstate\\nquestions should be submitted by  September 24, 2024 , and\\nthe whole proposal should be submitted on  September 30, 2024 . \\nThere are no specified dates for project deliverables,\\nbut the proposal should outline a timeline for project phases,\\nsuch as pre-design, design, content migration, and maintenance.\\nThe RFP does not specify the exact number of staff required, it emphasizes the need for personnel with relevant \\nskills in web design, development, and maintenance. The selected firm must provide training for up to 10 airport \\nemployees starting from the go-live date, indicating that the team should have the capability to train and \\nsupport airport staff in website management.\\nThe RFP does not outline specific pricing compliance requirements.\\n RFP does not explicitly mention the requirement for specific legal and regulatory documentation, such as \\ninsurance or licenses, within the proposal. \\nRFP does not specify any mandatory environmental or sustainability requirements.\\nIt does not state any explicit DEI obligations. However, given the increasing focus on diversity in public contracts,\\n firms may wish to highlight their commitment to DEI in their proposal to strengthen their bid.\\nIt does not specify that client reference checks or past performance verifications are required.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(documents)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_index(documents, db_path):\n",
    "    if not os.path.exists(db_path):\n",
    "        os.makedirs(db_path)\n",
    "\n",
    "    # dimensions of text-ada-embedding-002\n",
    "    d = 384\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "    \n",
    "    # save index to disk\n",
    "    index.storage_context.persist(persist_dir=db_path)\n",
    "    print(f\"Vector DB created and saved to {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created and saved to ./hash\n"
     ]
    }
   ],
   "source": [
    "save_faiss_index(documents, r\"./hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f707811bc90> >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 384\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaissVectorStore(stores_text=False, is_embedding_query=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7f707811b200>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7f707808a420>, vector_stores={'default': FaissVectorStore(stores_text=False, is_embedding_query=True), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x7f7078089eb0>, property_graph_store=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7f7071201dc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
